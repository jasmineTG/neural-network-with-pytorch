{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "my_copy_of _GNN_03B-Full GNN for graph prediction",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE8-UiVLebZo"
      },
      "source": [
        "# Preamble\n",
        "This section downloads and install a conda environment on your colab virtual machine, which makes dealing with dependencies a lot easier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvohFD_uenKH"
      },
      "source": [
        "## Installing condacolab\n",
        "We will use a package called condacolab which will download an initialize a conda environment which we can later install packages to"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h77ZMyN_wWaO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25ffc430-b94a-44c0-80f1-9c9e30b72666"
      },
      "source": [
        "#Preamble, this will install a conda (mamba actually) environment on your Colab VM, which really makes working with complex dependencies (RDKit in this case) easier.\n",
        "# YOU WILL LIKELY GET A NOTIFICATION ABOUT YOUR SESSION CRASHING, THIS IS EXPECTED BEHAVIOUR (the install will restart the python kernel). \n",
        "# Wait until this cell is done before running the rest of the notebook.\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ¨ðŸ°âœ¨ Everything looks OK!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZr98jGBmmaC",
        "outputId": "e7d27dfa-f87e-417a-a705-75b5b7f71f5b"
      },
      "source": [
        "# Check that we now have a working conda environment. You should get the output \"Everything looks OK!\"\n",
        "import condacolab\n",
        "condacolab.check()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ¨ðŸ°âœ¨ Everything looks OK!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9ddFxnYezCN"
      },
      "source": [
        "## Installing the notebook packages\n",
        "Now that condacolab is up and running, we install the packages. To install the correct version of cuda toolkits  for pytorch, we first check which version of cuda is installed on the VM. If the last line is not `cuda_11.1.xxx`, you need to change the mamba install line to match the version you see from `nvcc`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIOHL4ZagFLq",
        "outputId": "ad1087b0-375e-4c6e-aa1c-af18dc08e00b"
      },
      "source": [
        "!nvcc --version  # Check what cuda version is installed, this must match the cudatoolkit=XY.Z we give to the mamba install line"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyHApbORm8eS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c83ba8aa-26d7-44da-ca27-34b022387d62"
      },
      "source": [
        "# Install the required packages. Note that condacolab uses mamba (a conda \n",
        "# reimplementation) by default. This will likely take between 5-10 minutes.\n",
        "!mamba install pytorch cudatoolkit=11.1 rdkit -c pytorch -c conda-forge > /dev/null\n",
        "print(\"Done installing packages\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done installing packages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ5rwsF5vKEW"
      },
      "source": [
        "# Graph Neural Network Encoder\n",
        "\n",
        "This code repeats the code from the GNN Encoder notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2QH_wF5g3nb"
      },
      "source": [
        "from collections import defaultdict\n",
        "from collections.abc import Set\n",
        "\n",
        "import rdkit\n",
        "from rdkit.Chem import MolFromSmiles\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "from rdkit.Chem import Draw\n",
        "IPythonConsole.ipython_useSVG=True  #< set this to False if you want PNGs instead of SVGs\n",
        "IPythonConsole.drawOptions.addAtomIndices = True  # This will help when looking at the Mol graph representation\n",
        "IPythonConsole.molSize = 600, 600\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "float_type = torch.float32  # We're hardcoding types in the tensors further down\n",
        "categorical_type = torch.long\n",
        "mask_type = torch.float32  # We're going to be multiplying our internal calculations with a mask using this type\n",
        "labels_type = torch.float32 # We're going to use BCEWithLogitsLoss, which expects the labels to be of the same type as the predictions"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVhtCXz2Lpbb"
      },
      "source": [
        "class ContinuousFeature:\n",
        "  def __init__(self, name):\n",
        "    self.name = name\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f'<ContinuousFeature: {self.name}>'\n",
        "\n",
        "  def __eq__(self, other):\n",
        "    return self.name == other.name\n",
        "\n",
        "  def __hash__(self):\n",
        "    return hash(self.name)\n",
        "\n",
        "class CategoricalFeature:\n",
        "  def __init__(self, name, values, add_null_value=True):\n",
        "    self.name = name\n",
        "    self.has_null_value = add_null_value\n",
        "    if self.has_null_value:\n",
        "      self.null_value = None\n",
        "      values = (None,) + tuple(values)\n",
        "    self.values = tuple(values)\n",
        "    self.value_to_idx_mapping = {v: i for i, v in enumerate(values)}\n",
        "    self.inv_value_to_idx_mapping = {i: v for v, i in self.value_to_idx_mapping.items()}\n",
        "    \n",
        "    if self.has_null_value:\n",
        "      self.null_value_idx = self.value_to_idx_mapping[self.null_value]\n",
        "  \n",
        "  def get_null_idx(self):\n",
        "    if self.has_null_value:\n",
        "      return self.null_value_idx\n",
        "    else:\n",
        "      raise RuntimeError(f\"Categorical variable {self.name} has no null value\")\n",
        "\n",
        "  def value_to_idx(self, value):\n",
        "    return self.value_to_idx_mapping[value]\n",
        "  \n",
        "  def idx_to_value(self, idx):\n",
        "    return self.inv_value_to_idx_mapping[idx]\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.values)\n",
        "  \n",
        "  def __repr__(self):\n",
        "    return f'<CategoricalFeature: {self.name}>'\n",
        "\n",
        "  def __eq__(self, other):\n",
        "    return self.name == other.name and self.values == other.values\n",
        "\n",
        "  def __hash__(self):\n",
        "    return hash((self.name, self.values))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CIiwUfjTjua"
      },
      "source": [
        "ATOM_SYMBOLS = ['H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne', 'Na', \n",
        "                'Mg', 'Al', 'Si', 'P', 'S', 'Cl', 'Ar', 'K', 'Ca', 'Sc', 'Ti', \n",
        "                'V', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn', 'Ga', 'Ge', 'As', \n",
        "                'Se', 'Br', 'Kr', 'Rb', 'Sr', 'Y', 'Zr', 'Nb', 'Mo', 'Tc', 'Ru', \n",
        "                'Rh', 'Pd', 'Ag', 'Cd', 'In', 'Sn', 'Sb', 'Te', 'I', 'Xe', 'Cs', \n",
        "                'Ba', 'Hf', 'Ta', 'W', 'Re', 'Os', 'Ir', 'Pt', 'Au', 'Hg', 'Tl', \n",
        "                'Pb', 'Bi', 'Po', 'At', 'Rn', 'Fr', 'Ra', 'Rf', 'Db', 'Sg', \n",
        "                'Bh', 'Hs', 'Mt', 'Ds', 'Rg', 'Cn', 'Fl', 'Lv', 'La', 'Ce', 'Pr', \n",
        "                'Nd', 'Pm', 'Sm', 'Eu', 'Gd', 'Tb', 'Dy', 'Ho', 'Er', 'Tm', 'Yb', \n",
        "                'Lu', 'Ac', 'Th', 'Pa', 'U', 'Np', 'Pu', 'Am', 'Cm', 'Bk', \n",
        "                'Cf', 'Es', 'Fm', 'Md', 'No', 'Lr']\n",
        "ATOM_SYMBOLS_FEATURE = CategoricalFeature('atom_symbol', ATOM_SYMBOLS)\n",
        "\n",
        "ATOM_AROMATIC_VALUES = [True, False]\n",
        "ATOM_AROMATIC_FEATURE = CategoricalFeature('is_aromatic', ATOM_AROMATIC_VALUES)\n",
        "\n",
        "# In practice you might like to use categroical features for valence, but we use continuous here for demonstration\n",
        "ATOM_EXPLICIT_VALENCE_FEATURE = ContinuousFeature('explicit_valence')\n",
        "\n",
        "ATOM_IMPLICIT_VALENCE_FEATURE = ContinuousFeature('implicit_valence')\n",
        "\n",
        "ATOM_FEATURES = [ATOM_SYMBOLS_FEATURE, ATOM_AROMATIC_FEATURE, ATOM_EXPLICIT_VALENCE_FEATURE, ATOM_IMPLICIT_VALENCE_FEATURE]\n",
        "\n",
        "def get_atom_features(rd_atom):\n",
        "  atom_symbol = rd_atom.GetSymbol()\n",
        "  is_aromatic = rd_atom.GetIsAromatic()\n",
        "  implicit_valence = float(rd_atom.GetImplicitValence())\n",
        "  explicit_valence = float(rd_atom.GetExplicitValence())\n",
        "  return {ATOM_SYMBOLS_FEATURE: atom_symbol,\n",
        "          ATOM_AROMATIC_FEATURE: is_aromatic,\n",
        "          ATOM_EXPLICIT_VALENCE_FEATURE: explicit_valence,\n",
        "          ATOM_IMPLICIT_VALENCE_FEATURE: implicit_valence}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXWWFqbhMIqd"
      },
      "source": [
        "# We could use the RDKit enumeration types instead of strings, but the advantage\n",
        "# of doing it like this is that our representation becomes independent of RDKit\n",
        "BOND_TYPES = ['UNSPECIFIED', 'SINGLE', 'DOUBLE', 'TRIPLE', 'QUADRUPLE', \n",
        "              'QUINTUPLE', 'HEXTUPLE', 'ONEANDAHALF', 'TWOANDAHALF',\n",
        "              'THREEANDAHALF','FOURANDAHALF', 'FIVEANDAHALF', 'AROMATIC', \n",
        "              'IONIC', 'HYDROGEN', 'THREECENTER',\t'DATIVEONE', 'DATIVE',\n",
        "              'DATIVEL', 'DATIVER', 'OTHER', 'ZERO']\n",
        "TYPE_FEATURE = CategoricalFeature('bond_type', BOND_TYPES)\n",
        "\n",
        "BOND_DIRECTIONS = ['NONE', 'BEGINWEDGE', 'BEGINDASH', 'ENDDOWNRIGHT', 'ENDUPRIGHT', 'EITHERDOUBLE' ]\n",
        "DIRECTION_FEATURE = CategoricalFeature('bond_direction', BOND_DIRECTIONS)\n",
        "\n",
        "BOND_STEREO = ['STEREONONE', 'STEREOANY', 'STEREOZ', 'STEREOE', \n",
        "               'STEREOCIS', 'STEREOTRANS']\n",
        "STEREO_FEATURE = CategoricalFeature('bond_stereo', BOND_STEREO)\n",
        "\n",
        "AROMATIC_VALUES = [True, False]\n",
        "AROMATIC_FEATURE = CategoricalFeature('is_aromatic', AROMATIC_VALUES)\n",
        "\n",
        "BOND_FEATURES = [TYPE_FEATURE, DIRECTION_FEATURE, AROMATIC_FEATURE, STEREO_FEATURE]\n",
        "\n",
        "def get_bond_features(rd_bond):\n",
        "  bond_type = str(rd_bond.GetBondType())\n",
        "  bond_stereo_info = str(rd_bond.GetStereo())\n",
        "  bond_direction = str(rd_bond.GetBondDir())\n",
        "  is_aromatic = rd_bond.GetIsAromatic()\n",
        "  return {TYPE_FEATURE: bond_type,\n",
        "          DIRECTION_FEATURE: bond_direction,\n",
        "          AROMATIC_FEATURE: is_aromatic,\n",
        "          STEREO_FEATURE: bond_stereo_info}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04crgcXvhI9X"
      },
      "source": [
        "def rdmol_to_graph(mol):\n",
        "  atoms = {rd_atom.GetIdx(): get_atom_features(rd_atom) for rd_atom in mol.GetAtoms()}\n",
        "  bonds = {frozenset((rd_bond.GetBeginAtomIdx(), rd_bond.GetEndAtomIdx())): get_bond_features(rd_bond) for rd_bond in mol.GetBonds()}\n",
        "  return atoms, bonds"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9CzqW5uyopl"
      },
      "source": [
        "def smiles_to_graph(smiles):\n",
        "  rd_mol = MolFromSmiles(smiles)\n",
        "  graph = rdmol_to_graph(rd_mol)\n",
        "  return graph"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxyM_A9fyxSc"
      },
      "source": [
        "g = smiles_to_graph('c1ccccc1')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JJ3Q2-bje4m"
      },
      "source": [
        "class GraphDataset(Dataset):\n",
        "  def __init__(self, *, graphs, labels, node_variables, edge_variables, metadata=None):\n",
        "    '''\n",
        "    Create a new graph dataset, \n",
        "    '''\n",
        "    self.graphs = graphs\n",
        "    self.labels = labels\n",
        "    assert len(self.graphs) == len(self.labels), \"The graphs and labels lists must be the same length\"\n",
        "    self.metadata = metadata\n",
        "    if self.metadata is not None:\n",
        "      assert len(self.metadata) == len(self.graphs), \"The metadata list needs to be as long as the graphs\"\n",
        "    self.node_variables = node_variables\n",
        "    self.edge_variables = edge_variables\n",
        "    self.categorical_node_variables = [var for var in self.node_variables if isinstance(var, CategoricalFeature)]\n",
        "    self.continuous_node_variables = [var for var in self.node_variables if isinstance(var, ContinuousFeature)]\n",
        "    self.categorical_edge_variables = [var for var in self.edge_variables if isinstance(var, CategoricalFeature)]\n",
        "    self.continuous_edge_variables = [var for var in self.edge_variables if isinstance(var, ContinuousFeature)]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.graphs)\n",
        "\n",
        "  def make_continuous_node_features(self, nodes):\n",
        "    if len(self.continuous_node_variables) == 0:\n",
        "      return None\n",
        "    n_nodes = len(nodes)\n",
        "    n_features = len(self.continuous_node_variables)\n",
        "    continuous_node_features = torch.zeros((n_nodes, n_features), dtype=float_type)\n",
        "    for node_idx, features in nodes.items():\n",
        "      node_features = torch.tensor([features[continuous_feature] for continuous_feature in self.continuous_node_variables], dtype=float_type)\n",
        "      continuous_node_features[node_idx] = node_features\n",
        "    return continuous_node_features\n",
        "      \n",
        "  def make_categorical_node_features(self, nodes):\n",
        "    if len(self.categorical_node_variables) == 0:\n",
        "      return None\n",
        "    n_nodes = len(nodes)\n",
        "    n_features = len(self.categorical_node_variables)\n",
        "    categorical_node_features = torch.zeros((n_nodes, n_features), dtype=categorical_type)\n",
        "\n",
        "    for node_idx, features in nodes.items():\n",
        "      for i, categorical_variable in enumerate(self.categorical_node_variables):\n",
        "          value = features[categorical_variable]\n",
        "          value_index = categorical_variable.value_to_idx(value)\n",
        "          categorical_node_features[node_idx, i] = value_index\n",
        "\n",
        "    return categorical_node_features\n",
        "\n",
        "  def make_continuous_edge_features(self, n_nodes, edges):\n",
        "    if len(self.continuous_edge_variables) == 0:\n",
        "      return None\n",
        "    n_features = len(self.continuous_edge_variables)\n",
        "    continuous_edge_features = torch.zeros((n_nodes, n_nodes, n_features), dtype=float_type)\n",
        "    for edge, features in edges.items():\n",
        "      edge_features = torch.tensor([features[continuous_feature] for continuous_feature in self.continuous_edge_variables], dtype=float_type)\n",
        "      u,v = edge\n",
        "      continuous_edge_features[u, v] = edge_features\n",
        "      if isinstance(edge, Set):\n",
        "        continuous_edge_features[v, u] = edge_features\n",
        "\n",
        "    return continuous_edge_features\n",
        "\n",
        "  def make_categorical_edge_features(self, n_nodes, edges):\n",
        "    if len(self.categorical_edge_variables) == 0:\n",
        "      return None\n",
        "    n_features = len(self.categorical_edge_variables)\n",
        "    categorical_edge_features = torch.zeros((n_nodes, n_nodes, n_features), dtype=categorical_type)\n",
        "\n",
        "    for edge, features in edges.items():\n",
        "      u,v = edge\n",
        "      for i, categorical_variable in enumerate(self.categorical_edge_variables):\n",
        "          value = features[categorical_variable]\n",
        "          value_index = categorical_variable.value_to_idx(value)\n",
        "          categorical_edge_features[u, v, i] = value_index\n",
        "          if isinstance(edge, Set):\n",
        "            categorical_edge_features[v, u, i] = value_index\n",
        "\n",
        "    return categorical_edge_features\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    # This is where the important stuff happens. We use our node and \n",
        "    # edge variable attributes to select what node and edge features to use.\n",
        "    # In practice, we often do this as a pre-processing step, but here we do it \n",
        "    # in the getitem function for clarity\n",
        "\n",
        "    graph = self.graphs[index]\n",
        "    nodes, edges = graph\n",
        "    n_nodes = len(nodes)\n",
        "    continuous_node_features = self.make_continuous_node_features(nodes)\n",
        "    categorical_node_features = self.make_categorical_node_features(nodes)\n",
        "    continuous_edge_features = self.make_continuous_edge_features(n_nodes, edges)\n",
        "    categorical_edge_features = self.make_categorical_edge_features(n_nodes, edges)\n",
        "\n",
        "    label = self.labels[index]\n",
        "\n",
        "    nodes_idx = sorted(nodes.keys())\n",
        "    edge_list = sorted(edges.keys())\n",
        "\n",
        "    n_nodes = len(nodes)\n",
        "    adjacency_matrix = torch.zeros((n_nodes, n_nodes), dtype=float_type)\n",
        "    for edge in edges:\n",
        "      u, v = edge\n",
        "      adjacency_matrix[u,v] = 1\n",
        "      if isinstance(edge, Set):\n",
        "        # This edge is unordered, assume this is a undirected graph\n",
        "        adjacency_matrix[v,u] = 1\n",
        "\n",
        "    adjacency_list = defaultdict(list)\n",
        "    for edge in edges:\n",
        "      u,v = edge\n",
        "      adjacency_list[u].append(v)\n",
        "      # Assume undirected graph is the edge is a set\n",
        "      if isinstance(edge, Set):\n",
        "        adjacency_list[v].append(u)\n",
        "\n",
        "    data_record = {'nodes': nodes_idx,\n",
        "                   'adjacency_matrix': adjacency_matrix,\n",
        "                   'adjacency_list': adjacency_list,\n",
        "                   'categorical_node_features': categorical_node_features,\n",
        "                   'continuous_node_features': continuous_node_features,\n",
        "                   'categorical_edge_features': categorical_edge_features,\n",
        "                   'continuous_edge_features': continuous_edge_features,\n",
        "                   'label': label}\n",
        "\n",
        "    # If you need to add extra information (metadata about this graph) you can \n",
        "    # add an extra key-value pair here. The advantage of using a dict compared \n",
        "    # to a tuple is that the downstreams code doesn't break as long as at least \n",
        "    # the expected keys are present. The downside is that using a dict adds \n",
        "    # overhead (accessing a dict compared to unpacking a tuple).\n",
        "    # A more robust implementation might actually make a separate class for \n",
        "    # dataset entires\n",
        "    if self.metadata is not None:\n",
        "      data_record['metadata'] = self.metadata[index]\n",
        "    return data_record\n",
        "\n",
        "  def get_node_variables(self):\n",
        "    return {'continuous': self.continuous_node_variables,\n",
        "            'categorical': self.categorical_node_variables}\n",
        "  \n",
        "  def get_edge_variables(self):\n",
        "    return {'continuous': self.continuous_edge_variables,\n",
        "            'categorical': self.categorical_edge_variables}"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFJdou8MmjEu"
      },
      "source": [
        "def make_molecular_graph_dataset(smiles_records, atom_features=ATOM_FEATURES, bond_features=BOND_FEATURES):\n",
        "  '''\n",
        "  Create a new GraphDataset from a list of smiles_records dictionaries.\n",
        "  These records should contain the key 'smiles' and 'label'. Any other keys will be saved as a 'metadata' record.\n",
        "  '''\n",
        "  graphs = []\n",
        "  labels = []\n",
        "  metadata = []\n",
        "  for smiles_record in smiles_records:\n",
        "    smiles = smiles_record['smiles']\n",
        "    label = smiles_record['label']\n",
        "    graph = smiles_to_graph(smiles)\n",
        "    graphs.append(graph)\n",
        "    labels.append(label)\n",
        "    metadata.append(smiles_record)\n",
        "  return GraphDataset(graphs=graphs, \n",
        "                      labels=labels, \n",
        "                      node_variables=atom_features, \n",
        "                      edge_variables=bond_features, \n",
        "                      metadata=metadata)\n",
        "  "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_jNqe7gLrRX"
      },
      "source": [
        "import torch\n",
        "from torch.nn import Embedding, Module, ModuleList"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy2k-LB2LvAp"
      },
      "source": [
        "class Embedder(Module):\n",
        "  def __init__(self, categorical_variables, embedding_dim):\n",
        "    super().__init__()\n",
        "    self.categorical_variables = categorical_variables\n",
        "    embeddings = []\n",
        "    for var in categorical_variables:\n",
        "      num_embeddings = len(var)\n",
        "      if var.has_null_value:\n",
        "        # It's not uncommon to have missing values, we support this assinging a special 0-index which have the zero-vector as its embedding\n",
        "        embedding = Embedding(num_embeddings, embedding_dim, padding_idx=var.get_null_idx())\n",
        "      else:\n",
        "        embedding = Embedding(num_embeddings, embedding_dim)\n",
        "      embeddings.append(embedding)\n",
        "    self.embeddings = ModuleList(embeddings)\n",
        "    \n",
        "  \n",
        "  def forward(self, categorical_features):\n",
        "    # The node features is a matrix with as many rows as nodes of our graph\n",
        "    # and as many columns as we have categorical features\n",
        "    all_embedded_vars = []\n",
        "    for i, embedding in enumerate(self.embeddings):\n",
        "      # We pick out just the i'th column. The ellipsis '...' in a numpy-style \n",
        "      # slice is a useful way of saying you want full range over all other axises\n",
        "      # We use it so that this can actually take a categorical_features array\n",
        "      # with arbitrary number of trailing axises to support both the node \n",
        "      # features, the edge features and the mini-batched version of both\n",
        "      var_indices = categorical_features[..., i]  \n",
        "      embedded_vars = embedding(var_indices)\n",
        "      all_embedded_vars.append(embedded_vars)\n",
        "\n",
        "    # If you like, you can implement concatenation instead of sum here\n",
        "    stacked_embedded_vars = torch.stack(all_embedded_vars, dim=0)\n",
        "    embedded_vars = torch.sum(stacked_embedded_vars, dim=0)\n",
        "    return embedded_vars"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqoEnLmETVcJ"
      },
      "source": [
        "class FeatureCombiner(Module):\n",
        "  def __init__(self, categorical_variables, embedding_dim):\n",
        "    super().__init__()\n",
        "    self.categorical_variables = categorical_variables\n",
        "    self.embedder = Embedder(self.categorical_variables, embedding_dim)\n",
        "    \n",
        "  def forward(self, continuous_features, categorical_features, ):\n",
        "    # We need to be agnostic to whether we have categorical features and continuous features (it's not uncommon to only use one kind)\n",
        "    features = []\n",
        "    if categorical_features is not None:\n",
        "      embedded_features = self.embedder(categorical_features)\n",
        "      features.append(embedded_features)\n",
        "      # The embedded features are now of shape (n_nodes, embedding_dim)\n",
        "    if continuous_features is not None:\n",
        "      features.append(continuous_features)\n",
        "    if len(features) == 0:\n",
        "      raise RuntimeError('No features to combine')\n",
        "    full_features = torch.cat(features, dim=-1)  # Now we concatenate along the feature dimension\n",
        "    return full_features\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N10j3k2YiLA"
      },
      "source": [
        "from collections.abc import Set # We assume that edges as sets are for undirected graphs\n",
        "\n",
        "def collate_graph_batch(batch):\n",
        "  '''Collate a batch of graph dictionaries produdce by a GraphDataset'''\n",
        "  batch_size = len(batch)\n",
        "\n",
        "  max_nodes = max(len(graph['nodes']) for graph in batch)\n",
        "  \n",
        "  # We start by allocating the tensors we'll use. We defer allocating feature\n",
        "  # tensors until we know the graphs actually has those kinds of features.\n",
        "  adjacency_matrices = torch.zeros((batch_size, max_nodes, max_nodes), dtype=float_type)\n",
        "  labels = torch.tensor([graph['label'] for graph in batch], dtype=labels_type)\n",
        "  stacked_continuous_node_features = None\n",
        "  stacked_categorical_node_features = None\n",
        "  stacked_continuous_edge_features = None\n",
        "  stacked_categorical_edge_features = None\n",
        "\n",
        "  nodes_mask = torch.zeros((batch_size, max_nodes), dtype=mask_type)\n",
        "  edge_mask = torch.zeros((batch_size, max_nodes, max_nodes), dtype=mask_type)\n",
        "  \n",
        "  has_metadata = False\n",
        "\n",
        "  for i, graph in enumerate(batch):\n",
        "    if 'metadata' in graph:\n",
        "      has_metadata = True\n",
        "    # We'll take basic information about the different graphs from the adjacency \n",
        "    # matrix\n",
        "    adjacency_matrix = graph['adjacency_matrix']\n",
        "    g_nodes, g_nodes = adjacency_matrix.shape\n",
        "    adjacency_matrices[i, :g_nodes, :g_nodes] = adjacency_matrix\n",
        "\n",
        "    # Now when we know how many of the entries are valid, we set those to 1s in\n",
        "    # the masks\n",
        "    edge_mask[i, :g_nodes, :g_nodes] = 1\n",
        "    nodes_mask[i, :g_nodes] = 1\n",
        "    \n",
        "\n",
        "    # All the feature constructions follow the same recipie. We essentially\n",
        "    # locate the entries in the stacked feature tensor (containing all graphs)\n",
        "    # and set it with the features from the current graph.\n",
        "    g_continuous_node_features = graph['continuous_node_features']\n",
        "    if g_continuous_node_features is not None:\n",
        "      if stacked_continuous_node_features is None:\n",
        "        g_nodes, num_features = g_continuous_node_features.shape\n",
        "        stacked_continuous_node_features = torch.zeros((batch_size, max_nodes, num_features))\n",
        "      stacked_continuous_node_features[i, :g_nodes] = g_continuous_node_features\n",
        "    \n",
        "    g_categorical_node_features = graph['categorical_node_features']\n",
        "    if g_categorical_node_features is not None:\n",
        "      if stacked_categorical_node_features is None:\n",
        "        g_nodes, num_features = g_categorical_node_features.shape\n",
        "        stacked_categorical_node_features = torch.zeros((batch_size, max_nodes, num_features), dtype=categorical_type)\n",
        "      stacked_categorical_node_features[i, :g_nodes] = g_categorical_node_features\n",
        "\n",
        "    g_continuous_edge_features = graph['continuous_edge_features']\n",
        "    if g_continuous_edge_features is not None:\n",
        "      if stacked_continuous_edge_features is None:\n",
        "        g_nodes, g_nodes, num_features = g_continuous_edge_features.shape\n",
        "        stacked_continuous_edge_features = torch.zeros((batch_size, max_nodes, max_nodes, num_features))\n",
        "      stacked_continuous_edge_features[i, :g_nodes, :g_nodes] = g_continuous_edge_features\n",
        "\n",
        "    g_categorical_edge_features = graph['categorical_edge_features']\n",
        "    if g_categorical_edge_features is not None:\n",
        "      if stacked_categorical_edge_features is None:\n",
        "        g_nodes, g_nodes, num_features = g_categorical_edge_features.shape\n",
        "        stacked_categorical_edge_features = torch.zeros((batch_size, max_nodes, max_nodes, num_features), dtype=categorical_type)\n",
        "      stacked_categorical_edge_features[i, :g_nodes, :g_nodes] = g_categorical_edge_features\n",
        "\n",
        "\n",
        "  batch_record = {'adjacency_matrices': adjacency_matrices,\n",
        "          'categorical_node_features': stacked_categorical_node_features,\n",
        "          'continuous_node_features': stacked_continuous_node_features,\n",
        "          'categorical_edge_features': stacked_categorical_edge_features,\n",
        "          'continuous_edge_features': stacked_continuous_edge_features,\n",
        "          'nodes_mask': nodes_mask,\n",
        "          'edge_mask': edge_mask,\n",
        "          'labels': labels}\n",
        "  if has_metadata:\n",
        "    batch_record['metadata'] = [g['metadata'] for g in batch]\n",
        "\n",
        "  return batch_record\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RI0wbOQhTwm"
      },
      "source": [
        "from torch.nn import Module, Embedding, ModuleList, Linear, Sequential, ReLU, LayerNorm, Dropout\n",
        "from torch.nn.functional import layer_norm, dropout\n",
        "\n",
        "class BasicGNNConfig:\n",
        "  def __init__(self, *, \n",
        "               d_model: int, \n",
        "               n_layers: int, \n",
        "               ffn_dim: int):\n",
        "    self.d_model = d_model\n",
        "    self.n_layers = n_layers\n",
        "    self.ffn_dim = ffn_dim\n",
        "    \n",
        "class BasicGraphLayer(Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()    \n",
        "    self.config = config\n",
        "    self.input_dim = config.d_model\n",
        "    self.output_dim = config.d_model\n",
        "    self.ffn_dim = config.ffn_dim\n",
        "    self.neighbour_edges_mlp = Sequential(Linear(self.input_dim, self.ffn_dim), \n",
        "                                          ReLU(), \n",
        "                                          Linear(self.ffn_dim, self.output_dim))\n",
        "    self.center_mlp = Sequential(Linear(self.input_dim, self.ffn_dim), \n",
        "                                 ReLU(), \n",
        "                                 Linear(self.ffn_dim, self.output_dim))\n",
        "    self.output_mlp = Sequential(Linear(self.input_dim, self.ffn_dim), \n",
        "                                 ReLU(), \n",
        "                                 Linear(self.ffn_dim, self.output_dim))\n",
        "  \n",
        "  def forward(self, adjacency_matrix, node_features, edge_features, node_mask, edge_mask):\n",
        "    center_updated_node_features = self.center_mlp(node_features)\n",
        "    edge_and_node_features = edge_features + node_features.unsqueeze(dim=-2)\n",
        "    neighbourhood = self.neighbour_edges_mlp(edge_and_node_features)\n",
        "\n",
        "    masked_edge_and_node_features = neighbourhood * adjacency_matrix.unsqueeze(dim=-1)\n",
        "    masked_edge_and_node_features = masked_edge_and_node_features * edge_mask.unsqueeze(dim=-1)\n",
        "\n",
        "    reduced_neighbourhoods = masked_edge_and_node_features.sum(dim=2)\n",
        "    \n",
        "    aggregated_neighbourhoods = reduced_neighbourhoods + center_updated_node_features\n",
        "    updated_node_features = self.output_mlp(aggregated_neighbourhoods)\n",
        "    masked_updated_features = updated_node_features * node_mask.unsqueeze(dim=-1)\n",
        "    return masked_updated_features\n",
        "\n",
        "\n",
        "class BasicGraphEncoder(torch.nn.Module):\n",
        "  def __init__(self, *,\n",
        "               config: BasicGNNConfig, \n",
        "               continuous_node_variables=None,\n",
        "               categorical_node_variables=None,\n",
        "               continuous_edge_variables=None,\n",
        "               categorical_edge_variables=None,\n",
        "               layer_type=BasicGraphLayer):\n",
        "    super().__init__()\n",
        "\n",
        "    self.config = config\n",
        "    self.layer_type = layer_type\n",
        "\n",
        "    self.continuous_node_variables = continuous_node_variables\n",
        "    self.categorical_node_variables = categorical_node_variables\n",
        "    self.continuous_edge_variables = continuous_edge_variables\n",
        "    self.categorical_edge_variables = categorical_edge_variables\n",
        "\n",
        "    # We want the embeddings together with the continuous values to be of dimension d_model, therefore the allocate d_model - len(continuous_variables) as the embeddings dim\n",
        "    self.categorical_node_embeddings_dim = config.d_model - len(self.continuous_node_variables)\n",
        "    self.categorical_edge_embeddings_dim = config.d_model - len(self.continuous_edge_variables)\n",
        "\n",
        "    self.node_featurizer = FeatureCombiner(self.categorical_node_variables, \n",
        "                                           self.categorical_node_embeddings_dim)\n",
        "    self.edge_featurizer = FeatureCombiner(self.categorical_edge_variables, \n",
        "                                           self.categorical_edge_embeddings_dim)\n",
        "    \n",
        "    # Notice that we use the supplied layer type above when creating the graph\n",
        "    # layers. This allows us to easily change the kind of graph layers\n",
        "    # we use later on\n",
        "    self.graph_layers = ModuleList([layer_type(config) for l in range(config.n_layers)])\n",
        "    \n",
        "  def forward(self, batch):\n",
        "    # First order of business is to embed the node embeddings\n",
        "    node_mask = batch['nodes_mask']\n",
        "    batch_size, max_nodes = node_mask.shape\n",
        "    \n",
        "    continuous_node_features = batch['continuous_node_features']\n",
        "    categorical_node_features = batch['categorical_node_features']\n",
        "    node_features = self.node_featurizer(continuous_node_features, categorical_node_features)\n",
        "    masked_node_features = node_features * node_mask.unsqueeze(-1)\n",
        "    \n",
        "    continuous_edge_features = batch['continuous_edge_features']\n",
        "    categorical_edge_features = batch['categorical_edge_features']\n",
        "    edge_features = self.edge_featurizer(continuous_edge_features, categorical_edge_features)\n",
        "    edge_mask = batch['edge_mask']\n",
        "    masked_edge_features = edge_features * edge_mask.unsqueeze(-1)\n",
        "\n",
        "    # We have now embedded the node features, we'll propagate them through our \n",
        "    # graph layers\n",
        "    adjacency_matrix = batch['adjacency_matrices']\n",
        "    memory_state = masked_node_features\n",
        "    for l in self.graph_layers:\n",
        "      memory_state = l(adjacency_matrix, memory_state, masked_edge_features , node_mask, edge_mask)\n",
        "\n",
        "    return memory_state"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmq5WYY0qanr"
      },
      "source": [
        "# A complete GNN\n",
        "Now that we've seen how a Graph Neural Network encoder works, we will go through how to combine it with a prediction head to perform graph prediction .We'll use the same molecules as in the previous notebook (the BBBP dataset).\n",
        "\n",
        "Your task will be to take the GNN Encoder defined above and combine it with a prediction head to train the network. See the _task_ section below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aTiODPVv1th"
      },
      "source": [
        "\n",
        "## The dataset\n",
        "We will be using the same BBBP dataset as in the preivous notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fRkH33ZnS5f"
      },
      "source": [
        "# Well start by downloading the dataset. We're relying on the direct download link from MoleculeNet\n",
        "! wget -q https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/BBBP.csv"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tmk0hB3snikU"
      },
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict, deque # We'll use this to construct the dataset splits\n",
        "from rdkit.Chem.Scaffolds.MurckoScaffold import MurckoScaffoldSmilesFromSmiles"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "9vKIE5_yqwHA",
        "outputId": "469e9569-be0a-4d41-cf4b-5b0151015172"
      },
      "source": [
        "bbbp_table = pd.read_csv('BBBP.csv')\n",
        "bbbp_table"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num</th>\n",
              "      <th>name</th>\n",
              "      <th>p_np</th>\n",
              "      <th>smiles</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Propanolol</td>\n",
              "      <td>1</td>\n",
              "      <td>[Cl].CC(C)NCC(O)COc1cccc2ccccc12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Terbutylchlorambucil</td>\n",
              "      <td>1</td>\n",
              "      <td>C(=O)(OC(C)(C)C)CCCc1ccc(cc1)N(CCCl)CCCl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>40730</td>\n",
              "      <td>1</td>\n",
              "      <td>c12c3c(N4CCN(C)CC4)c(F)cc1c(c(C(O)=O)cn2C(C)CO...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "      <td>C1CCN(CC1)Cc1cccc(c1)OCCCNC(=O)C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>cloxacillin</td>\n",
              "      <td>1</td>\n",
              "      <td>Cc1onc(c2ccccc2Cl)c1C(=O)N[C@H]3[C@H]4SC(C)(C)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2045</th>\n",
              "      <td>2049</td>\n",
              "      <td>licostinel</td>\n",
              "      <td>1</td>\n",
              "      <td>C1=C(Cl)C(=C(C2=C1NC(=O)C(N2)=O)[N+](=O)[O-])Cl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2046</th>\n",
              "      <td>2050</td>\n",
              "      <td>ademetionine(adenosyl-methionine)</td>\n",
              "      <td>1</td>\n",
              "      <td>[C@H]3([N]2C1=C(C(=NC=N1)N)N=C2)[C@@H]([C@@H](...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2047</th>\n",
              "      <td>2051</td>\n",
              "      <td>mesocarb</td>\n",
              "      <td>1</td>\n",
              "      <td>[O+]1=N[N](C=C1[N-]C(NC2=CC=CC=C2)=O)C(CC3=CC=...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2048</th>\n",
              "      <td>2052</td>\n",
              "      <td>tofisoline</td>\n",
              "      <td>1</td>\n",
              "      <td>C1=C(OC)C(=CC2=C1C(=[N+](C(=C2CC)C)[NH-])C3=CC...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2049</th>\n",
              "      <td>2053</td>\n",
              "      <td>azidamfenicol</td>\n",
              "      <td>1</td>\n",
              "      <td>[N+](=NCC(=O)N[C@@H]([C@H](O)C1=CC=C([N+]([O-]...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2050 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       num  ...                                             smiles\n",
              "0        1  ...                   [Cl].CC(C)NCC(O)COc1cccc2ccccc12\n",
              "1        2  ...           C(=O)(OC(C)(C)C)CCCc1ccc(cc1)N(CCCl)CCCl\n",
              "2        3  ...  c12c3c(N4CCN(C)CC4)c(F)cc1c(c(C(O)=O)cn2C(C)CO...\n",
              "3        4  ...                   C1CCN(CC1)Cc1cccc(c1)OCCCNC(=O)C\n",
              "4        5  ...  Cc1onc(c2ccccc2Cl)c1C(=O)N[C@H]3[C@H]4SC(C)(C)...\n",
              "...    ...  ...                                                ...\n",
              "2045  2049  ...    C1=C(Cl)C(=C(C2=C1NC(=O)C(N2)=O)[N+](=O)[O-])Cl\n",
              "2046  2050  ...  [C@H]3([N]2C1=C(C(=NC=N1)N)N=C2)[C@@H]([C@@H](...\n",
              "2047  2051  ...  [O+]1=N[N](C=C1[N-]C(NC2=CC=CC=C2)=O)C(CC3=CC=...\n",
              "2048  2052  ...  C1=C(OC)C(=CC2=C1C(=[N+](C(=C2CC)C)[NH-])C3=CC...\n",
              "2049  2053  ...  [N+](=NCC(=O)N[C@@H]([C@H](O)C1=CC=C([N+]([O-]...\n",
              "\n",
              "[2050 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IV2AD_Bynk6"
      },
      "source": [
        "We'll now filter the dataset and create the kind of data our dataset building function expects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oncEnL6Ywfhn",
        "outputId": "0fe8afff-904c-4c9c-e64c-d09e57d7dc95"
      },
      "source": [
        "# There are about 50 problematic SMILES, RDKit will give you about as many ERRORs \n",
        "# and WARNINGs, this is expected behaviour. We supress these warnings here\n",
        "# 11 of the SMILES can't be parsed at all\n",
        "from rdkit import RDLogger\n",
        "RDLogger.DisableLog('rdApp.*')  \n",
        "\n",
        "smiles_records = []\n",
        "for i, num, name, p_np, smiles in bbbp_table.to_records():\n",
        "  # check if RDKit accepts this smiles\n",
        "  if MolFromSmiles(smiles) is not None:\n",
        "    smiles_record = {'smiles': smiles, 'label': p_np, 'metadata': {'row': i}}\n",
        "    smiles_records.append(smiles_record)\n",
        "  else:\n",
        "    print(f'Molecule {smiles} on row {i} could not be parsed by RDKit')\n",
        "  "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Molecule O=N([O-])C1=C(CN=C1NCCSCc2ncccc2)Cc3ccccc3 on row 59 could not be parsed by RDKit\n",
            "Molecule c1(nc(NC(N)=[NH2])sc1)CSCCNC(=[NH]C#N)NC on row 61 could not be parsed by RDKit\n",
            "Molecule Cc1nc(sc1)\\[NH]=C(\\N)N on row 391 could not be parsed by RDKit\n",
            "Molecule s1cc(CSCCN\\C(NC)=[NH]\\C#N)nc1\\[NH]=C(\\N)N on row 614 could not be parsed by RDKit\n",
            "Molecule c1c(c(ncc1)CSCCN\\C(=[NH]\\C#N)NCC)Br on row 642 could not be parsed by RDKit\n",
            "Molecule n1c(csc1\\[NH]=C(\\N)N)c1ccccc1 on row 645 could not be parsed by RDKit\n",
            "Molecule n1c(csc1\\[NH]=C(\\N)N)c1cccc(c1)N on row 646 could not be parsed by RDKit\n",
            "Molecule n1c(csc1\\[NH]=C(\\N)N)c1cccc(c1)NC(C)=O on row 647 could not be parsed by RDKit\n",
            "Molecule n1c(csc1\\[NH]=C(\\N)N)c1cccc(c1)N\\C(NC)=[NH]\\C#N on row 648 could not be parsed by RDKit\n",
            "Molecule s1cc(nc1\\[NH]=C(\\N)N)C on row 649 could not be parsed by RDKit\n",
            "Molecule c1(cc(N\\C(=[NH]\\c2cccc(c2)CC)C)ccc1)CC on row 685 could not be parsed by RDKit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gXgktX55YH4"
      },
      "source": [
        "We will create some \"canonical\" splits we can use in the remainder of the notebook. These are just random selections of the molecules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gjVLCOJyhQr"
      },
      "source": [
        "import random\n",
        "random.seed(1729)\n",
        "\n",
        "training_fraction = 0.8\n",
        "dev_fraction = 0.1\n",
        "n_examples = len(smiles_records)\n",
        "n_training_examples = int(n_examples*training_fraction)\n",
        "n_dev_examples = int(n_examples*dev_fraction)\n",
        "\n",
        "indices = list(range(n_examples))\n",
        "random.shuffle(indices)  # shuffle is in place\n",
        "training_indices = indices[:n_training_examples]\n",
        "dev_indices = indices[n_training_examples:n_training_examples+n_dev_examples]\n",
        "test_indices = indices[n_training_examples+n_dev_examples:]\n",
        "\n",
        "training_smiles_records = [smiles_records[i] for i in training_indices]\n",
        "dev_smiles_records = [smiles_records[i] for i in dev_indices]\n",
        "test_smiles_records = [smiles_records[i] for i in test_indices]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OM-kOaPtKrsI"
      },
      "source": [
        "training_graph_dataset = make_molecular_graph_dataset(training_smiles_records)\n",
        "dev_graph_dataset = make_molecular_graph_dataset(dev_smiles_records)\n",
        "test_graph_dataset = make_molecular_graph_dataset(test_smiles_records)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b64FDfZsOWB"
      },
      "source": [
        "## The training loop\n",
        "To make training models a bit easier, we encapsulate most of the training loop and its state in a Trainer class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFi9AezswiNr"
      },
      "source": [
        "from tqdm.notebook import tqdm, trange\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ePjtAzpwiNr",
        "outputId": "02dcd3bf-cbf5-4891-e8f0-1a2c9cd51cfa"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "print(\"Device is\", device)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device is cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxlNWD6WUwyh"
      },
      "source": [
        "from tqdm.notebook import tqdm, trange\n",
        "from torch.nn import BCEWithLogitsLoss, MSELoss\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, mean_absolute_error, median_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def batch_to_device(batch, device):\n",
        "  moved_batch = {}\n",
        "  for k, v in batch.items():\n",
        "    if torch.is_tensor(v):\n",
        "      v = v.to(device)\n",
        "    moved_batch[k] = v\n",
        "  return moved_batch\n",
        "\n",
        "class Trainer:\n",
        "  def __init__(self, *, model, \n",
        "               loss_fn, training_dataloader, \n",
        "               dev_dataloader, device=device):\n",
        "    self.model = model\n",
        "    self.training_dataloader = training_dataloader\n",
        "    self.dev_dataloader = dev_dataloader\n",
        "    self.device = device\n",
        "    self.model.to(device)\n",
        "    self.total_epochs = 0\n",
        "    self.optimizer = AdamW(self.model.parameters(), lr=1e-4)\n",
        "    self.loss_fn = loss_fn\n",
        "\n",
        "  def train(self, epochs):\n",
        "    with trange(epochs, desc='Epoch', position=0) as epoch_progress:\n",
        "      batches_per_epoch = len(self.training_dataloader) + len(self.dev_dataloader)\n",
        "      for epoch in epoch_progress:\n",
        "        train_loss = 0\n",
        "        train_n = 0\n",
        "        for i, training_batch in enumerate(tqdm(self.training_dataloader, desc='Training batch', leave=False)):\n",
        "          self.optimizer.zero_grad()\n",
        "          # Move all tensors to the device\n",
        "          self.model.train()\n",
        "          training_batch = batch_to_device(training_batch, self.device)\n",
        "          prediction = self.model(training_batch)\n",
        "          labels = training_batch['labels']\n",
        "          loss = self.loss_fn(prediction.squeeze(), labels) # By default the predictions have shape (batch_size, 1)\n",
        "          loss.backward()\n",
        "          self.optimizer.step()\n",
        "          batch_n = len(labels)\n",
        "          train_loss += batch_n * loss.cpu().item()\n",
        "          train_n += batch_n\n",
        "        #print(f\"Training loss for epoch {total_epochs}\", train_loss/train_n)\n",
        "        self.total_epochs += 1\n",
        "\n",
        "        dev_predictions = []\n",
        "        dev_labels = []\n",
        "        dev_n = 0\n",
        "        dev_loss = 0\n",
        "        for i, dev_batch in enumerate(tqdm(self.dev_dataloader, desc=\"Dev batch\", leave=False)):\n",
        "          self.model.eval()\n",
        "          with torch.no_grad():\n",
        "            dev_batch = batch_to_device(dev_batch, self.device)\n",
        "            prediction = self.model(dev_batch).squeeze()\n",
        "            dev_predictions.extend(prediction.tolist())\n",
        "            labels = dev_batch['labels']\n",
        "            dev_labels.extend(labels.tolist())\n",
        "            loss = self.loss_fn(prediction, labels) # By default the predictions have shape (batch_size, 1)\n",
        "            batch_n = len(labels)\n",
        "            dev_loss += batch_n*loss.cpu().item()\n",
        "            dev_n += batch_n\n",
        "        epoch_progress.set_description(f\"Epoch: train loss {train_loss/train_n: .3f}, dev loss {dev_loss/dev_n: .3f}\")\n",
        "\n",
        "\n",
        "def evaluate_model(trainer, dataloader, label=None, hue_order=[0,1]):\n",
        "  eval_predictions = []\n",
        "  eval_labels = []\n",
        "  eval_loss = 0\n",
        "  eval_n = 0\n",
        "  model = trainer.model\n",
        "  loss_fn = trainer.loss_fn\n",
        "  total_epochs = trainer.total_epochs\n",
        "  for i, eval_batch in enumerate(tqdm(dataloader, desc='batch')):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      eval_batch = batch_to_device(eval_batch, device)\n",
        "      prediction = model(eval_batch).squeeze()\n",
        "      eval_predictions.extend(prediction.tolist())\n",
        "      labels = eval_batch['labels']\n",
        "      eval_labels.extend(labels.tolist())\n",
        "      loss = loss_fn(prediction, labels) # By default the predictions have shape (batch_size, 1)\n",
        "      batch_n = len(labels)\n",
        "      eval_loss += batch_n*loss.cpu().item()\n",
        "      eval_n += batch_n\n",
        "  average_loss = eval_loss/eval_n\n",
        "  roc_auc = roc_auc_score(eval_labels, eval_predictions)\n",
        "  eval_df = pd.DataFrame(data={'target': eval_labels, 'predictions': eval_predictions})\n",
        "  sns.kdeplot(data=eval_df, x='predictions', hue='target', hue_order=hue_order)\n",
        "  sns.rugplot(data=eval_df, x='predictions', hue='target', hue_order=hue_order)\n",
        "  \n",
        "  if label is not None:\n",
        "    title = f\"{label} dataset after {total_epochs} epochs\\nloss {average_loss}\\nROC AUC {roc_auc}\"\n",
        "  else:\n",
        "    title = f\"After {total_epochs} epochs\\nloss {average_loss}\\nROC AUC {roc_auc}\"\n",
        "  plt.title(title)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DLUOLQJxVOg"
      },
      "source": [
        "# Task 1\n",
        "\n",
        "Now that we've prepared the dataset and GNN Encoder, your task is flesh out the GraphPredictionHead and combine it with the GNNEncoder to train a network for Graph Prediction on the BBBP dataset. The main challange is to take the output of the Encoder, a tensor with a `max_nodes` axis and reduce this to having only a single feature vector per graph in the batch. Look to the final task of the previous notebook.\n",
        "\n",
        "**Implement the reductions of the output of the Encoder and combine the encoder with the prediction head in the `GraphPredictionNeuralNetwork` class and train the neural network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a5wBTtqhbwt"
      },
      "source": [
        "class GraphPredictionHeadConfig(BasicGNNConfig):\n",
        "  def __init__(self, *, pooling_type='sum', **kwargs):\n",
        "    # Pooling type can be 'sum' or 'mean'\n",
        "    super().__init__(**kwargs)\n",
        "    self.pooling_type = pooling_type\n",
        "\n",
        "class GraphPredictionHead(Module):\n",
        "  def __init__(self, input_dim, output_dim, config):\n",
        "    super().__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.config = config\n",
        "    self.predictor = Sequential(Linear(self.input_dim, self.config.ffn_dim), \n",
        "                                ReLU(), \n",
        "                                Linear(self.config.ffn_dim, self.output_dim))\n",
        "\n",
        "  def forward(self, node_features, node_mask):\n",
        "    # You need to take the node_features, a tensor of shape \n",
        "    # (*leading_axises, max_nodes, d_model) and reduce this either by 'sum' or\n",
        "    # 'mean'. You can assume these features are valid, i.e. that they have been\n",
        "    # masked at a previous step. Assign the result to the name *pooled_nodes*\n",
        "    if self.config.pooling_type == 'sum':\n",
        "      # Implement 'sum' based pooling here\n",
        "      pooled_nodes = ...\n",
        "    elif self.config.pooling_type == 'mean':\n",
        "      # Implement 'mean' based pooling here\n",
        "      ...\n",
        "    else:\n",
        "      raise ValueError(f'Unsupported pooling type {self.config.pooling_type}')\n",
        "    \n",
        "    prediction = self.predictor(pooled_nodes)\n",
        "    return prediction"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpLUo3Jp662d"
      },
      "source": [
        "class GraphPredictionNeuralNetwork(Module):\n",
        "  def __init__(self, config, dataset, \n",
        "               output_dim=1,\n",
        "               graph_layer_type=BasicGraphLayer):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.encoder = BasicGraphEncoder(config=config,\n",
        "                                     continuous_node_variables=dataset.continuous_node_variables,\n",
        "                                     categorical_node_variables=dataset.categorical_node_variables,\n",
        "                                     continuous_edge_variables=dataset.continuous_edge_variables,\n",
        "                                     categorical_edge_variables=dataset.categorical_edge_variables,\n",
        "                                     layer_type=graph_layer_type)\n",
        "    \n",
        "    self.prediction_head = GraphPredictionHead(input_dim=config.d_model, \n",
        "                                               output_dim=output_dim, config=config)\n",
        "  def forward(self, batch):\n",
        "    # Here you should combine the BasicGraphEncoder with the GraphPredictionHead.\n",
        "    # assign the output of the head to the name *prediction*\n",
        "    ...\n",
        "    return prediction"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQo_R_Kfkd37"
      },
      "source": [
        "batch_size=32\n",
        "num_dataloader_workers=2 # The colab instances are very limited in number of cpus\n",
        "\n",
        "training_dataloader = DataLoader(training_graph_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=True, \n",
        "                                          num_workers=num_dataloader_workers, \n",
        "                                          collate_fn=collate_graph_batch)\n",
        "dev_dataloader = DataLoader(dev_graph_dataset, \n",
        "                                     batch_size=batch_size, \n",
        "                                     shuffle=False, \n",
        "                                     num_workers=num_dataloader_workers, \n",
        "                                     drop_last=False, \n",
        "                                     collate_fn=collate_graph_batch)\n",
        "test_dataloader = DataLoader(test_graph_dataset, \n",
        "                                     batch_size=batch_size, \n",
        "                                     shuffle=False, \n",
        "                                     num_workers=num_dataloader_workers, \n",
        "                                     drop_last=False, \n",
        "                                     collate_fn=collate_graph_batch)\n",
        "torch.manual_seed(1729)\n",
        "config = GraphPredictionHeadConfig(d_model=32, n_layers=4,\n",
        "                        ffn_dim=32, \n",
        "                        pooling_type='sum')\n",
        "model = GraphPredictionNeuralNetwork(config=config, dataset=training_graph_dataset)\n",
        "loss_fn = BCEWithLogitsLoss()\n",
        "trainer = Trainer(model=model, \n",
        "                  loss_fn=loss_fn, \n",
        "                  training_dataloader=training_dataloader,\n",
        "                  dev_dataloader=dev_dataloader)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvOUPADDwiNr"
      },
      "source": [
        "batch_size=32\n",
        "num_dataloader_workers=2 # The colab instances are very limited in number of cpus\n",
        "\n",
        "training_dataloader = DataLoader(training_graph_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=True, \n",
        "                                          num_workers=num_dataloader_workers, \n",
        "                                          collate_fn=collate_graph_batch)\n",
        "dev_dataloader = DataLoader(dev_graph_dataset, \n",
        "                                     batch_size=batch_size, \n",
        "                                     shuffle=False, \n",
        "                                     num_workers=num_dataloader_workers, \n",
        "                                     drop_last=False, \n",
        "                                     collate_fn=collate_graph_batch)\n",
        "test_dataloader = DataLoader(test_graph_dataset, \n",
        "                                     batch_size=batch_size, \n",
        "                                     shuffle=False, \n",
        "                                     num_workers=num_dataloader_workers, \n",
        "                                     drop_last=False, \n",
        "                                     collate_fn=collate_graph_batch)\n",
        "torch.manual_seed(1729)\n",
        "config = GraphPredictionHeadConfig(d_model=32, n_layers=4,\n",
        "                        ffn_dim=32, \n",
        "                        pooling_type='sum')\n",
        "model = GraphPredictionNeuralNetwork(config=config, dataset=training_graph_dataset)\n",
        "loss_fn = BCEWithLogitsLoss()\n",
        "trainer = Trainer(model=model, \n",
        "                  loss_fn=loss_fn, \n",
        "                  training_dataloader=training_dataloader,\n",
        "                  dev_dataloader=dev_dataloader)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV7S5WBjaWr8"
      },
      "source": [
        "trainer.train(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DNDbrpV1m2V"
      },
      "source": [
        "#Task 2\n",
        "\n",
        "It has been observed that Graph Neural Networks suffer from depth, the neighbourhood aggregation ends ups diffusing the information in the network.\n",
        "\n",
        "A simple remedy of this is to use residual connections. This is a way of constructing deep networks where the output of a transforming layer (e.g. a linear layer followed by a nonlinear function) is just added to the input to that layer. If the transform block is an MLP ($f_{MLP}()$) we could write it like:\n",
        "\n",
        "$$\n",
        "f_{residual}(\\mathbf{x}) = f_{MLP}(\\mathbf{x}) + \\mathbf{x}\n",
        "$$\n",
        "\n",
        "This makes GNNs not suffer from these depth issues\n",
        "\n",
        "**Task: Implement residual connections in the GraphEncoder. You can do it in the `forward()` of the Encoder or in\n",
        "the GNN layer. Note that we should do this for all GNN layers**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Does the performance of the network change when you add residual connections?**\n",
        "\n",
        "**Note: for this to work, the dimensionality of the input vector must be the same as the output of the layer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hpOMxs45cnn"
      },
      "source": [
        "**Either** implement your change in the `ResdiualGraphLayer`or the `ResidualGraphEncoder` below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKNOFkpr6I-P"
      },
      "source": [
        "## In the graph layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnL_j-Cm5Lpa"
      },
      "source": [
        "from torch.nn import Module, Embedding, ModuleList, Linear, Sequential, ReLU, LayerNorm, Dropout\n",
        "from torch.nn.functional import layer_norm, dropout\n",
        "\n",
        "class ResidualGraphLayer(BasicGraphLayer):\n",
        "  def forward(self, adjacency_matrix, node_features, edge_features, node_mask, edge_mask):\n",
        "    # Implement the residual connection here\n",
        "\n",
        "    center_updated_node_features = self.center_mlp(node_features)\n",
        "    edge_and_node_features = edge_features + node_features.unsqueeze(dim=-2)\n",
        "    neighbourhood = self.neighbour_edges_mlp(edge_and_node_features)\n",
        "\n",
        "    masked_edge_and_node_features = neighbourhood * adjacency_matrix.unsqueeze(dim=-1)\n",
        "    masked_edge_and_node_features = masked_edge_and_node_features * edge_mask.unsqueeze(dim=-1)\n",
        "\n",
        "    reduced_neighbourhoods = masked_edge_and_node_features.sum(dim=2)\n",
        "    \n",
        "    aggregated_neighbourhoods = reduced_neighbourhoods + center_updated_node_features\n",
        "    updated_node_features = self.output_mlp(aggregated_neighbourhoods)\n",
        "    masked_updated_features = updated_node_features * node_mask.unsqueeze(dim=-1)\n",
        "    return masked_updated_features"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni392rAB7Q9Q"
      },
      "source": [
        "batch_size=32\n",
        "num_dataloader_workers=2 # The colab instances are very limited in number of cpus\n",
        "\n",
        "training_dataloader = DataLoader(training_graph_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=True, \n",
        "                                          num_workers=num_dataloader_workers, \n",
        "                                          collate_fn=collate_graph_batch)\n",
        "dev_dataloader = DataLoader(dev_graph_dataset, \n",
        "                                     batch_size=batch_size, \n",
        "                                     shuffle=False, \n",
        "                                     num_workers=num_dataloader_workers, \n",
        "                                     drop_last=False, \n",
        "                                     collate_fn=collate_graph_batch)\n",
        "test_dataloader = DataLoader(test_graph_dataset, \n",
        "                                     batch_size=batch_size, \n",
        "                                     shuffle=False, \n",
        "                                     num_workers=num_dataloader_workers, \n",
        "                                     drop_last=False, \n",
        "                                     collate_fn=collate_graph_batch)\n",
        "torch.manual_seed(1729)\n",
        "config = GraphPredictionHeadConfig(d_model=32, n_layers=4,\n",
        "                        ffn_dim=32, \n",
        "                        pooling_type='sum')\n",
        "model = GraphPredictionNeuralNetwork(config, training_graph_dataset, graph_layer_type=ResidualGraphLayer)\n",
        "loss_fn = BCEWithLogitsLoss()\n",
        "trainer = Trainer(model=model, \n",
        "                  loss_fn=loss_fn, \n",
        "                  training_dataloader=training_dataloader,\n",
        "                  dev_dataloader=dev_dataloader)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naJE23JW7Q9Q"
      },
      "source": [
        "trainer.train(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcmrU0Jy6MY5"
      },
      "source": [
        "## In the encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN3sD6_35rS3"
      },
      "source": [
        "class ResidualGraphEncoder(BasicGraphEncoder):\n",
        "  def forward(self, batch):\n",
        "    \n",
        "    node_mask = batch['nodes_mask']\n",
        "    batch_size, max_nodes = node_mask.shape\n",
        "    \n",
        "    continuous_node_features = batch['continuous_node_features']\n",
        "    categorical_node_features = batch['categorical_node_features']\n",
        "    node_features = self.node_featurizer(continuous_node_features, categorical_node_features)\n",
        "    masked_node_features = node_features * node_mask.unsqueeze(-1)\n",
        "    \n",
        "    continuous_edge_features = batch['continuous_edge_features']\n",
        "    categorical_edge_features = batch['categorical_edge_features']\n",
        "    edge_features = self.edge_featurizer(continuous_edge_features, categorical_edge_features)\n",
        "    edge_mask = batch['edge_mask']\n",
        "    masked_edge_features = edge_features * edge_mask.unsqueeze(-1)\n",
        "\n",
        "    adjacency_matrix = batch['adjacency_matrices']\n",
        "    memory_state = masked_node_features\n",
        "    for l in self.graph_layers:\n",
        "      # Implement the residual connection here\n",
        "      memory_state = l(adjacency_matrix, memory_state, masked_edge_features , node_mask, edge_mask)\n",
        "\n",
        "    return memory_state\n",
        "\n",
        "class ResidualGraphPredictionNeuralNetwork(GraphPredictionNeuralNetwork):\n",
        "  def __init__(self, config, dataset, \n",
        "               output_dim=1,\n",
        "               graph_layer_type=BasicGraphLayer):\n",
        "    super().__init__(config, dataset)\n",
        "    self.config = config\n",
        "    self.encoder = ResidualGraphEncoder(config=config,\n",
        "                                     continuous_node_variables=dataset.continuous_node_variables,\n",
        "                                     categorical_node_variables=dataset.categorical_node_variables,\n",
        "                                     continuous_edge_variables=dataset.continuous_edge_variables,\n",
        "                                     categorical_edge_variables=dataset.categorical_edge_variables,\n",
        "                                     layer_type=graph_layer_type)\n",
        "    \n",
        "    self.prediction_head = GraphPredictionHead(input_dim=config.d_model, \n",
        "                                               output_dim=output_dim, config=config)\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqphkQQv6H0N"
      },
      "source": [
        "batch_size=32\n",
        "num_dataloader_workers=2 # The colab instances are very limited in number of cpus\n",
        "\n",
        "training_dataloader = DataLoader(training_graph_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=True, \n",
        "                                          num_workers=num_dataloader_workers, \n",
        "                                          collate_fn=collate_graph_batch)\n",
        "dev_dataloader = DataLoader(dev_graph_dataset, \n",
        "                                     batch_size=batch_size, \n",
        "                                     shuffle=False, \n",
        "                                     num_workers=num_dataloader_workers, \n",
        "                                     drop_last=False, \n",
        "                                     collate_fn=collate_graph_batch)\n",
        "test_dataloader = DataLoader(test_graph_dataset, \n",
        "                                     batch_size=batch_size, \n",
        "                                     shuffle=False, \n",
        "                                     num_workers=num_dataloader_workers, \n",
        "                                     drop_last=False, \n",
        "                                     collate_fn=collate_graph_batch)\n",
        "torch.manual_seed(1729)\n",
        "config = GraphPredictionHeadConfig(d_model=32, n_layers=4,\n",
        "                        ffn_dim=32, \n",
        "                        pooling_type='sum')\n",
        "model = ResidualGraphPredictionNeuralNetwork(config, training_graph_dataset)\n",
        "loss_fn = BCEWithLogitsLoss()\n",
        "trainer = Trainer(model=model, \n",
        "                  loss_fn=loss_fn, \n",
        "                  training_dataloader=training_dataloader,\n",
        "                  dev_dataloader=dev_dataloader)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IhMAH9Y6H0O"
      },
      "source": [
        "trainer.train(2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}