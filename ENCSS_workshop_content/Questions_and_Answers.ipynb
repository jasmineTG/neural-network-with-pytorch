{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Questions, answers and information\n",
    "\n",
    "**Question:** Where will the zoom recordings be made available?\n",
    "**Answer:** We will make them available on the enccs.se website in the \"Training resources\". Expect this to take a week or two \n",
    "\n",
    "### October 21, Morning Session\n",
    "\n",
    "---\n",
    "\n",
    "**Question:** How do I start a GPU instance on Colab?\n",
    "**Answer:** Go to Runtime->Change runtime type\n",
    "\n",
    "---------\n",
    "\n",
    "**Question:** Why should one hot enconding be avoided in NN?\n",
    "**Answer:** There actually is a notebook on this linked in the overview for this session. It's optional, so don't worry if you don't get to looking into it. The upshot is that the representation is inefficient and will result in a lot of zeros in the gradients when training the neural network.\n",
    "\n",
    "--------\n",
    "\n",
    "**Question:** is the input order of vectors important? Shouldn't be, correct? \n",
    "**Answer:** Exactly right, in this problem we want to be _invariant_ of order, permutation-invariant so have to make sure our neural network is as well. This will covered in session 3.\n",
    "\n",
    "--------\n",
    "**Question:** I think we did too early a break out - now what? Break out session 3 at least.\n",
    "**Answer:** The idea now is that you go through the notebooks together, best if is one of you share the screen and you go through the notebook in sync.\n",
    "\n",
    "--------\n",
    "**Question:** What do you mean that summing the vectors is preferable to concatenation, aren't these very different things?\n",
    "\n",
    "**Question:** Could you repeat/clarify the part in which you said summing up random vectors in high dimensional space and how is that the reason why summing is preferred to concatenation?\n",
    "\n",
    "**Answer:** Our claim that summing is better relies on three assumptions: that our embeddings free variables, are random and high dimensional. \n",
    "\n",
    "If they are, sums of these vectors actually retain a lot of the information about the constituents. We could pretty easily test whether a particular embedding vector is part of the sum by just performing a dot prodcut (or compute the cosine similarity) with the sum. If the vector was not part of that sum, we're likely to get a value close to 0. This is also what the downstreams matrix multiplication of our neural network can do, it can learn to have singular vectors aligned with the categorical values of interest.\n",
    "\n",
    "Another way of thinking about it is the example we use in the notebook: We can implement concatenation by first padding two vectors and then summing them. This shows that concatenation can be thought of as a special case of summing vectors\n",
    "\n",
    "There's a better argument for summing instead of concatenating if we also assume that this sum will be multiplied with a matrix (as will be the case in this workshop).\n",
    "In that case, lets say we concatenate the vectors of _free variables_ $\\mathbf{x}_1$ and $\\mathbf{x}_2$:\n",
    "\n",
    "$$\\mathbf{x} = \\begin{bmatrix} \\mathbf{x}_1 \\\\ \\mathbf{x}_2 \\end{bmatrix}$$\n",
    "\n",
    "Then the matrix multiplication $W \\mathbf{x}$ is the same as \n",
    "$$\n",
    "W \\mathbf{x} = W_1 \\mathbf{x}_1 + W_2 \\mathbf{x}_2\n",
    "$$\n",
    "\n",
    "Where $W = \\begin{bmatrix} W_1 & W_2 \\end{bmatrix}$\n",
    "\n",
    "If we use concatenation, the first layer that the concatenation encounters will essentially perform a sum of these concatenated vectors after they have been linearily transformed. \n",
    "\n",
    "Since all the matrices and vectors are freely parameterized, we might just set $W_1 = W_2$ and not loose anything in terms of expressivity, especially if we increase the dimensionality of $\\mathbf{x}_1$ and $\\mathbf{x}_2$ to match the one the concatenation would have had.\n",
    "\n",
    "One argument against this this could be if we would like our combination of vectors to _not_ be permutation invariant. Let's say the vectors we want to combine are actually a sequence, where each position in the sequence will be represented by a vector, but this might be the _same_ vector.\n",
    "\n",
    "\n",
    "S_1 = \"a white cat visited the house\"\n",
    "S_2 = \"a cat visited the white house\"\n",
    "\n",
    "In this case, concatenation makes more sense\n",
    "\n",
    "$$S_1 = \\begin{bmatrix} \\mathbf{x}_\\text{a} \\\\ \\mathbf{x}_\\text{white} \\\\ \\mathbf{x}_\\text{cat} \\\\ \\mathbf{x}_\\text{visited} \\\\ \\mathbf{x}_\\text{the} \\\\  \n",
    "\\mathbf{x}_\\text{house} \\\\  \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$S_2 = \\begin{bmatrix} \\mathbf{x}_\\text{a} \\\\ \\mathbf{x}_\\text{cat} \\\\ \\mathbf{x}_\\text{visited} \\\\ \\mathbf{x}_\\text{the} \\\\ \\mathbf{x}_\\text{white} \\\\ \n",
    "\\mathbf{x}_\\text{house} \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "$$\n",
    "W S_1 = W_1 \\mathbf{x}_\\text{a} + W_2\\mathbf{x}_\\text{white} + W_3\\mathbf{x}_\\text{cat} + W_4 \\mathbf{x}_\\text{visited} + W_5\\mathbf{x}_\\text{the} +  W_6\\mathbf{x}_\\text{house} \n",
    "$$\n",
    "\n",
    "$$\n",
    "W S_2 = W_1 \\mathbf{x}_\\text{a} + W_2\\mathbf{x}_\\text{cat} + W_3\\mathbf{x}_\\text{visited} + W_4 \\mathbf{x}_\\text{the} + W_5\\mathbf{x}_\\text{white} +  W_6\\mathbf{x}_\\text{house} \n",
    "$$\n",
    "\n",
    "\n",
    "So in this case, assuming that the $\\mathbf{x}$'s are fixed, the model can learn to _project_ them differently depending on position in the input concatenation. The word vector $W_5\\mathbf{x}_\\text{white}$ will be different from $W_2\\mathbf{x}_\\text{white}$, even though it's the same word.\n",
    "\n",
    "If we instead summed them, we could not separate these two sentence representations.\n",
    "\n",
    "\n",
    "**Question:** So, it works for high dimensional space. What do you consider as high dimensional space as a rule of thumb? In the examples we had for example dimension 4\n",
    "\n",
    "**Answer:**\n",
    "This is really a matter of how much capacity do we need to separate our vectors. If we only have 4 values in the categorical variable, a 4-dimensional embedding space would be able to create orthogonal representations for these. But if we randomly generate them we might need more to have this property of _almost_ orthognal vectors (let's say 10, pulling a number out of the air.)\n",
    "\n",
    "In the example we chose low values for clarity, in practice you would have more.\n",
    "\n",
    "The properties of random vectors being almost orthogonal to each other depends on how many random vectors we're considering in relation to the dimensionality. \n",
    "\n",
    "In natural language processing it was for a long time customary to use 300 as the embedding dimension of word vectors.\n",
    "\n",
    "I (Erik) would say that anything above 100 dimensions is high dimensional, but depending on context, 10 can be as well.\n",
    "\n",
    "In practice, we typically set the embedding dimension to the same as the vectors we pass through our neural network. It's often high enough that we don't have to worry about \"collisions\" in embedding space.\n",
    "\n",
    "-------\n",
    "\n",
    "**comment** the standard valence for C, O,  N would be 4, 2, 3 :D (sorry, couldn't help it)\n",
    "\n",
    "Yeah, the valence we calculate are just the \"explicit valence\", essentially the number of explicit bonds (including doubles etc.) in the molecules. Since the standard valences should be able to be learned from the atom symbol, we _hopefully_ don't have to explicitly include them.\n",
    "\n",
    "\n",
    "----------\n",
    "\n",
    "\n",
    "------\n",
    "\n",
    ":::info\n",
    "*Always ask questions at the very bottom of this document, right above this.*\n",
    "::: \n",
    "\n",
    "### October 21, Afternoon Session\n",
    "\n",
    "---\n",
    "\n",
    ":::info\n",
    "*Always ask questions at the very bottom of this document, right above this.*\n",
    "::: \n",
    "\n",
    "\n",
    "### October 22, Morning Session\n",
    "\n",
    "---\n",
    "\n",
    ":::info\n",
    "*Always ask questions at the very bottom of this document, right above this.*\n",
    "::: \n",
    "\n",
    "### October 22, Afternoon Session\n",
    "\n",
    "---\n",
    "\n",
    ":::info\n",
    "*Always ask questions at the very bottom of this document, right above this.*\n",
    "::: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
